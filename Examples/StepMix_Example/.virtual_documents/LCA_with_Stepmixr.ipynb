











update.packages()


devtools::install_github("Labo-Lacourse/StepMixR@DEV")


### Le notebook marche sur la version développement qui sera
### éventuellement sur CRAN.
### detach("package:stepmixr")
#devtools::install_github("Labo-Lacourse/stepmixr", "DEV")
library(stepmixr)
library(ggplot2)
library(dplyr, warn.conflicts = FALSE, quiet = TRUE)
theme_set(theme_bw() + theme(legend.position = "bottom"))





df <- iris[, 1:4]; target <- iris[, 5] 





options(jupyter.plot_mimetypes = "image/svg+xml",
       repr.plot.height = 4)

iris_by_type <- rbind(data.frame(type = "Sepal",Length = iris[,1], Width = iris[,2], 
                                 species = iris[,5]),
                      data.frame(type = "Petal",Length = iris[,3], Width = iris[,4],
                                 species = iris[,5]))

ggplot(iris_by_type, aes(Length, Width, color = species)) + 
    geom_point(size = 3) + facet_wrap(~type, scales = "free")





model <- stepmix(n_components=3, measurement="continuous", random_state=123)
fit1 <- fit(model, X = df)
#df





df['Class.Membership'] = predict(fit1, df[,1:4])





groups_mod1 <- sprintf("group %d (n = %d)", 1:3, table(df['Class.Membership']))
tab_mod1 <- table(df['Class.Membership'])
names(tab_mod1) <- groups_mod1
barplot(tab_mod1, col = c("red", "green", "blue"))





table(df[,'Class.Membership'], target)





post_prob_mod1 = predict_proba(fit1,df[,1:4])





post_prob_mod1[df[,'Class.Membership'] != (0:2)[target],] %>% 
    head %>% 
    round(3)





iris_by_type$Class_different <-  factor(df[,'Class.Membership'] != (0:2)[target], 
                                        labels = c("same", "different"))

ggplot(iris_by_type, aes(Length, Width, color = species, shape = Class_different)) + 
    geom_point(size = 3) + facet_wrap(~type, scales = "free")





fossil::rand.index(as.numeric(target), df[,'Class.Membership'])








binary_data = sapply(df[,1:4], function(x) (0:1)[cut(x,breaks = 2)])
head(binary_data)





model <- stepmix(n_components=3, measurement="binary",
                verbose=1, random_state=123)
fit1 <- fit(model, binary_data)
df['binary_pred'] <- predict(fit1, binary_data)





table(target, df[,'binary_pred'])
sprintf("Rand index = %.4f", fossil::rand.index(as.numeric(target), 
                                                df[,'binary_pred']))


binary_means = fit1$get_parameters()[["measurement"]][["pis"]]





data.frame(Prob = as.vector(binary_means),
           group = factor(rep(1:3, 4), labels = c("Group 1", "Group 2", "Group 3")),
           features = rep(1:4, c(3,3,3,3))) %>% 
    ggplot(aes(features,Prob, group = group, col = group)) + 
    geom_line() + geom_point()





categorical_data = sapply(df[, 1:4], function(x) (0:2)[cut(x,breaks = 3)])
head(categorical_data)





model = stepmix(n_components=3, measurement="categorical",
                verbose=1, random_state=123)
fit1 <- fit(model, categorical_data)
df[,'categorical_pred'] = predict(fit1, categorical_data)





df[, 'categorical_pred2'] <- c(0, 2, 1)[df[, 'categorical_pred'] + 1]





table(target, c(1,3,2)[df[, 'categorical_pred'] + 1])


fossil::rand.index(as.numeric(target), df[,'categorical_pred'])


rand_score_models <- data.frame(
    rand_index = c(fossil::rand.index(as.numeric(target), df[,'binary_pred']),
                   fossil::rand.index(as.numeric(target), df[,'categorical_pred']),
                   fossil::rand.index(as.numeric(target), df[,'categorical_pred'])))
rand_score_models$type = c("binary", "Categorical", "Continuous")


ggplot(rand_score_models, aes(type, rand_index)) + geom_point() + geom_line(group = 1, color = "blue")








## More complex models need a more complex description
### StepMix provides a function to quickly build mixed descriptors for DataFrames
df_mixed <- cbind(df[,1:2], Petal.Length = binary_data[,3], Petal.Width = categorical_data[,4]) 
### Unspecified variables are simply not included in mixed_data
md <- mixed_descriptor(data = df_mixed, continuous = 1:2, binary = 3, 
     categorical = 4)


head(df_mixed)


md$descriptor





# Pass descriptor to StepMix and fit model
model = stepmix(n_components=3, measurement=md$descriptor, random_state=as.integer(123))
# Fit model
mixed_fit <- fit(model, md$data)
# Class predictions
md$data['mixed_pred'] <- predict(mixed_fit, X = md$data)





table(md$data[,'mixed_pred'],iris$Species)


fossil::rand.index(as.numeric(target), md$data[,'mixed_pred'])








df_na <- as.matrix(df)
df_na[sample(1:600,120)] <- NaN 
df_na <- as.data.frame(df_na)


head(df_na[,1:4])





model <- stepmix(n_components=3, measurement="continuous_nan",
                verbose=1, random_state=123)
fit1 <- fit(model, df_na[,1:4])
df[,'continuous_pred_nan'] <- predict(fit1, df_na[,1:4])





table(target, df[,'continuous_pred_nan'])


fossil::rand.index(as.numeric(target), df[,'continuous_pred_nan'])








dbr <- data_bakk_response(n_samples=2000, sep_level=.9, random_state=42)
X <- dbr[[1]]; y <- dbr[[2]]; target <- dbr[[3]]


# On crée le modèle, on ajuste aux données X et Y.
model = stepmix(n_components=3, measurement='binary',
                structural='gaussian_unit', verbose=1,
                random_state=123)


fit1 <- fit(model, X, y)
summary(fit1)





preds = predict(fit1, X, y)
# J'ajoute +1 car R commence ses index à 1.
preds2 = c(0, 2, 1)[preds+1]


preds_tabs = table(target, preds2)
preds_tabs


preds_pgood = sum(diag(preds_tabs)) / sum(preds_tabs) * 100
sprintf("%.1f %% de bonne prédiction",preds_pgood)





fit1$get_parameters()['structural']





params <- as.table(fit1$get_parameters()[['structural']][['means']][,1])
names(params) <- c("Classe 1", "Classe 2", "Classe 3")


barplot(params, col = c("red", "green", "blue"), ylab = "Moyenne dans la classe")
abline(h = 0, lty = 2)








dbr <- data_bakk_covariate(n_samples=2000, sep_level=.9,
                           random_state=42)
X <- apply(dbr[[1]], 2, as.integer); y <- as.integer(dbr[[2]]); target <- dbr[[3]]





opt_params = list(
    method    = 'newton-raphson',  # Can also be "gradient",
    intercept = TRUE,
    max_iter  = as.integer(1)  # Number of opt. step each time we update the model
)





model = stepmix(n_components=3, measurement='binary',
                structural='covariate', structural_params=opt_params,
                verbose=1, random_state=123)
# Fit data
# Provide both measurement data X and structural data Y
fit1 <- fit(model, X, y)





preds = predict(fit1, X, y)
preds2 = c(2, 0, 1)[preds + 1]
table(target, preds2)





BETA = fit1$get_parameters()[['structural']][['beta']]
BETA


# On met la catégorie 1 comme référence.
BETA = BETA - c(1,1,1) %o% BETA[1,]





XX = cbind(1, y[1:5])
XX





la = XX %*% t(BETA)
exp(la)





PRB_APRIORI = exp(la) / matrix(apply(exp(la), 1, sum), 5, 3)








dbr = data_bakk_response(n_samples=2000, sep_level=.8, random_state=42)
X <- apply(dbr[[1]], 2, as.integer); y <- as.double(dbr[[2]]) ; target <- dbr[[3]]





model = stepmix(n_components=3, measurement='binary', n_steps=1,
                structural='gaussian_unit', verbose=0,
                random_state=123)
fit1 <- fit(model, X, y)
ll_1 = fit1$score(X, y)





model$n_steps = as.integer(2)
fit2 <- fit(model,X, y)
ll_2 = fit2$score(X, y)





model = stepmix(n_components=3, measurement='binary', n_steps=as.integer(3),
                structural='gaussian_unit', verbose=0,
                random_state=123)
y <- matrix(y, 2000, 1)
fit3 <- fit(model, X, y)
ll_3 <- fit3$score(X, y)


c(sprintf("1-step : %.4f", ll_1),
  sprintf("2-step : %.4f", ll_2),
  sprintf("3-step : %.4f", ll_3))





model = stepmix(n_components=3, measurement='binary', n_steps=3,
                structural='gaussian_unit', verbose=0,
                random_state=123)





result = expand.grid(c = c(NA, "BCH", "ML"), a = c("modal", "soft")) 



for(i in 1:6){
    if(is.na(result[i, "c"]))
        model$correction = NULL
    else
        model$correction = result[i, "c"]
    model$assignment = result[i, "a"]
    fitca <- fit(model, X, y)
    result[i,"ll"] <- fitca$score(X, y)
}






result








dbr = data_bakk_response(n_samples=2000, sep_level=.7, 
                         random_state=42)





model = stepmix(n_components=3, n_steps=1, measurement='bernoulli',
                verbose=0, structural='gaussian_unit', random_state=42)
fit1  <- fit(model, X, y)





mod_sel <- reticulate::py_run_string("from sklearn.model_selection import GridSearchCV, ParameterGrid")
grid = list(n_components = 1:8, n_steps = 1:3)  # une liste R est converti en dict python





gs <-  mod_sel$GridSearchCV(estimator=fit1, cv=as.integer(3), 
                            param_grid=grid)





gs$fit(X, y)





results <- gs$cv_results_
results.df <- data.frame(param_n_components = unlist(results[['param_n_components']]),
                         param_n_steps      = unlist(results[['param_n_steps']]),
                         mean_test_score    = results[['mean_test_score']])





ggplot(results.df, aes(param_n_components, y = mean_test_score, color = factor(param_n_steps))) + 
    geom_line(aes(group = param_n_steps), lwd = 2) + xlab("Nombre de classes") +
    labs(color = "Nombre d'étapes") + ylab("log-likelihood")


# Same model and grid as above
AIC <- BIC <- NULL
for(g in reticulate::iterate(mod_sel$ParameterGrid(grid))){
    model$n_components = g$n_components
    model$n_steps = g$n_steps
    fitg = fit(model, X, y)
    AIC <- c(AIC, fitg$aic(X, y))
    BIC <- c(BIC, fitg$bic(X, y))
}


# Save results to a dataframe
results.df <- cbind(results.df, AIC, BIC)





# AIC
ggplot(results.df, aes(param_n_components, y = AIC, 
                       color = factor(param_n_steps))) + 
  geom_line(aes(group = param_n_steps), lwd = 2) + 
  xlab("Nombre de classes") +
  labs(color = "Nombre d'étapes") + 
  ylab("Akaike information criterion")


# BIC
ggplot(results.df, aes(param_n_components, y = BIC, 
                       color = factor(param_n_steps))) + 
  geom_line(aes(group = param_n_steps), lwd = 2) + 
  xlab("Nombre de classes") +
  labs(color = "Nombre d'étapes") + 
  ylab("Bayes information criterion")








dbr <- data_bakk_response(n_samples=2000, sep_level=.7,
                          random_state=42)
X <- dbr[[1]]; y <- matrix(dbr[[2]], 2000, 1); target <- dbr[[3]]





model = stepmix(n_components=3, n_steps=1, measurement='bernoulli',
                structural='gaussian_unit', random_state=42)
fit1 <- fit(model, X, y)


# On peut extraire les paramètres.
params = fit1$get_parameters()


# Probabilité *a priori*
params['weights']








params[['measurement']][['pis']]








dbr <- data_bakk_response(n_samples=2000, sep_level=.9, random_state=42)
X <- dbr[[1]]; y <- matrix(dbr[[2]], 2000, 1); target <- dbr[[3]]






reticulate::py_run_string("from stepmix.stepmix import StepMix")

instr1 = 'model = StepMix(n_components=3, n_steps=1, measurement=\'bernoulli\',
                structural=\'gaussian_unit\', random_state=42, verbose=0)'

reticulate::py_run_string(instr1)

fit1 <- fit(model, X, y)





reticulate::py_run_string("from stepmix.bootstrap import bootstrap")
#boot <- reticulate::import("stepmix.bootstrap")
reticulate::py_set_attr(X)
help(py_set_attr)
library(reticulate)

r_to_py(X)
py$X = r_to_py(X)
py$y = r_to_py(y)
X = reticulate::r_to_py(X)
test = reticulate::py_run_string('model, bootstrapped_params = bootstrap(model, X, y, n_repetitions=1000)')
py$bootstrapped_params
result_boot <- boot$bootstrap(fit1, X, y, 
                              n_repetitions=as.integer(10)))
model, bootstrapped_params = bootstrap(model, X, y, n_repetitions=1000)

reticulate::py_run_string("test = 1")
test = reticulate::py_run_string("test")
test$test
reticulate::py_to_r(test)
result_boot
names(boot)
boot$GridSearchCV


bootstrapped_params['measurement']['pis']





Coef_table = pd.DataFrame(
    {'Est': np.mean(bootstrapped_params['measurement']['pis'],
                    axis=0).reshape(18, 1)[:, 0],
     'SE': np.std(bootstrapped_params['measurement']['pis'],
                  axis=0).reshape((18, 1))[:, 0]})


Coef_table['Est/SE'] = Coef_table['Est'] / Coef_table['SE']


Coef_table['P(<|t|)'] = 2 * norm.cdf(-np.abs(Coef_table['Est/SE']))


# Supposons qu'on veut calculer l'intervalle de confiance
# du premier paramètre
param_c0_response = bootstrapped_params['structural']['means'][:, 0, 0]


# On extrait les percentiles 2.5 et 97.5.
plow = np.percentile(param_c0_response, q=2.5)
phigh = np.percentile(param_c0_response, q=97.5)





plt.hist(param_c0_response, bins=100)
plt.xlabel("Mean 0")
plt.ylabel("Counts")
plt.bar(plow, height=300, width=.01, color="r")
plt.bar(phigh, height=300, width=.01, color="r")
plt.show()
print(f"\n95% Percentile CI: ({plow:.4f}, {phigh:.4f})")





# On créé les figures.
figures = plot_all_parameters_CI(model.get_parameters(),
                                 bootstrapped_params, alpha=5)
# et on les affiche.
for f in figures:
    f.subplots_adjust(top=0.85)  # Temp fix
